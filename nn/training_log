Iteration: 107750, Training Loss: -0.992178
Iteration: 107760, Training Loss: -0.995458
Iteration: 107770, Training Loss: -0.992388
Iteration: 107780, Training Loss: -0.991077
Iteration: 107790, Training Loss: -0.992335
Iteration: 107800, Training Loss: -0.990530
Iteration: 107810, Training Loss: -0.990177
Iteration: 107820, Training Loss: -0.993826
Iteration: 107830, Training Loss: -0.991035
Iteration: 107840, Training Loss: -0.992058
Iteration: 107850, Training Loss: -0.994194
Iteration: 107860, Training Loss: -0.990857
Iteration: 107870, Training Loss: -0.993911
Iteration: 107880, Training Loss: -0.991255
Iteration: 107890, Training Loss: -0.927650
Iteration: 107900, Training Loss: -0.988370
Iteration: 107910, Training Loss: -0.988506
Iteration: 107920, Training Loss: -0.988376
Iteration: 107930, Training Loss: -0.991121
Iteration: 107940, Training Loss: -0.985515
Iteration: 107950, Training Loss: -0.992836
Iteration: 107960, Training Loss: -0.983273
Iteration: 107970, Training Loss: -0.991327
Iteration: 107980, Training Loss: -0.995400
Iteration: 107990, Training Loss: -0.993327
Iteration: 108000, Training Loss: -0.992549
Iteration: 108010, Training Loss: -0.992583
Iteration: 108020, Training Loss: -0.995368
Iteration: 108030, Training Loss: -0.992532
Iteration: 108040, Training Loss: -0.989187
Iteration: 108050, Training Loss: -0.984847
Iteration: 108060, Training Loss: -0.995040
Iteration: 108070, Training Loss: -0.992127
Iteration: 108080, Training Loss: -0.993536
Iteration: 108090, Training Loss: -0.988301
Iteration: 108100, Training Loss: -0.988270
Iteration: 108110, Training Loss: -0.990928
Iteration: 108120, Training Loss: -0.991546
Iteration: 108130, Training Loss: -0.993945
Iteration: 108140, Training Loss: -0.992143
Iteration: 108150, Training Loss: -0.990343
Iteration: 108160, Training Loss: -0.993016
Iteration: 108170, Training Loss: -0.995568
Iteration: 108180, Training Loss: -0.985858
Iteration: 108190, Training Loss: -0.993243
Iteration: 108200, Training Loss: -0.996260
Iteration: 108210, Training Loss: -0.990725
Iteration: 108220, Training Loss: -0.993380
Iteration: 108230, Training Loss: -0.993331
Iteration: 108240, Training Loss: -0.994953
Iteration: 108250, Training Loss: -0.993070
Iteration: 108260, Training Loss: -0.992537
Iteration: 108270, Training Loss: -0.990361
Iteration: 108280, Training Loss: -0.991100
Iteration: 108290, Training Loss: -0.993838
Iteration: 108300, Training Loss: -0.989479
Iteration: 108310, Training Loss: -0.990735
Iteration: 108320, Training Loss: -0.993103
Iteration: 108330, Training Loss: -0.985053
Iteration: 108340, Training Loss: -0.993778
Iteration: 108350, Training Loss: -0.994037
Iteration: 108360, Training Loss: -0.993618
Iteration: 108370, Training Loss: -0.992015
Iteration: 108380, Training Loss: -0.986588
Iteration: 108390, Training Loss: -0.989657
Iteration: 108400, Training Loss: -0.987236
Iteration: 108410, Training Loss: -0.990290
Iteration: 108420, Training Loss: -0.990648
Iteration: 108430, Training Loss: -0.989917
Iteration: 108440, Training Loss: -0.988197
Iteration: 108450, Training Loss: -0.993855
Iteration: 108460, Training Loss: -0.990916
Iteration: 108470, Training Loss: -0.992670
Iteration: 108480, Training Loss: -0.990299
Iteration: 108490, Training Loss: -0.987580
Iteration: 108500, Training Loss: -0.989934
Iteration: 108510, Training Loss: -0.987614
Iteration: 108520, Training Loss: -0.994234
Iteration: 108530, Training Loss: -0.993251
Iteration: 108540, Training Loss: -0.995263
Iteration: 108550, Training Loss: -0.989757
Iteration: 108560, Training Loss: -0.993810
Iteration: 108570, Training Loss: -0.993460
Iteration: 108580, Training Loss: -0.991310
Iteration: 108590, Training Loss: -0.995153
Iteration: 108600, Training Loss: -0.993386
Iteration: 108610, Training Loss: -0.990966
Iteration: 108620, Training Loss: -0.993507
Iteration: 108630, Training Loss: -0.994544
Iteration: 108640, Training Loss: -0.993516
Iteration: 108650, Training Loss: -0.990176
Iteration: 108660, Training Loss: -0.994017
Iteration: 108670, Training Loss: -0.992211
Iteration: 108680, Training Loss: -0.994688
Iteration: 108690, Training Loss: -0.987481
Iteration: 108700, Training Loss: -0.993308
Iteration: 108710, Training Loss: -0.993240
Iteration: 108720, Training Loss: -0.991921
Iteration: 108730, Training Loss: -0.975666
Iteration: 108740, Training Loss: -0.993107
Iteration: 108750, Training Loss: -0.993763
Iteration: 108760, Training Loss: -0.993301
Iteration: 108770, Training Loss: -0.994430
Iteration: 108780, Training Loss: -0.996303
Iteration: 108790, Training Loss: -0.993423
Iteration: 108800, Training Loss: -0.993658
Iteration: 108810, Training Loss: -0.993204
Iteration: 108820, Training Loss: -0.990616
Iteration: 108830, Training Loss: -0.992805
Iteration: 108840, Training Loss: -0.994970
Iteration: 108850, Training Loss: -0.984556
Iteration: 108860, Training Loss: -0.991552
Iteration: 108870, Training Loss: -0.992061
Iteration: 108880, Training Loss: -0.809673
Iteration: 108890, Training Loss: -0.993455
Iteration: 108900, Training Loss: -0.988104
Iteration: 108910, Training Loss: -0.991738
Iteration: 108920, Training Loss: -0.988690
Iteration: 108930, Training Loss: -0.921575
Iteration: 108940, Training Loss: -0.992897
Iteration: 108950, Training Loss: -0.987212
Iteration: 108960, Training Loss: -0.992193
Iteration: 108970, Training Loss: -0.995385
Iteration: 108980, Training Loss: -0.992063
Iteration: 108990, Training Loss: -0.987421
Iteration: 109000, Training Loss: -0.991495
Iteration: 109010, Training Loss: -0.989218
Iteration: 109020, Training Loss: -0.992596
Iteration: 109030, Training Loss: -0.986775
Iteration: 109040, Training Loss: -0.992138
Iteration: 109050, Training Loss: -0.994246
Iteration: 109060, Training Loss: -0.990574
Iteration: 109070, Training Loss: -0.993083
Iteration: 109080, Training Loss: -0.991535
Iteration: 109090, Training Loss: -0.991322
Iteration: 109100, Training Loss: -0.991815
Iteration: 109110, Training Loss: -0.995247
Iteration: 109120, Training Loss: -0.885853
Iteration: 109130, Training Loss: -0.994421
Iteration: 109140, Training Loss: -0.982341
Iteration: 109150, Training Loss: -0.992178
Iteration: 109160, Training Loss: -0.992503
Iteration: 109170, Training Loss: -0.990663
Iteration: 109180, Training Loss: -0.991671
Iteration: 109190, Training Loss: -0.994741
Iteration: 109200, Training Loss: -0.995522
Iteration: 109210, Training Loss: -0.995232
Iteration: 109220, Training Loss: -0.990678
Iteration: 109230, Training Loss: -0.993879
Iteration: 109240, Training Loss: -0.993106
Iteration: 109250, Training Loss: -0.993615
Iteration: 109260, Training Loss: -0.994300
Iteration: 109270, Training Loss: -0.992167
Iteration: 109280, Training Loss: -0.995082
Iteration: 109290, Training Loss: -0.993833
Iteration: 109300, Training Loss: -0.995016
Iteration: 109310, Training Loss: -0.962354
Iteration: 109320, Training Loss: -0.989986
Iteration: 109330, Training Loss: -0.993187
Iteration: 109340, Training Loss: -0.993276
Iteration: 109350, Training Loss: -0.993349
Iteration: 109360, Training Loss: -0.994060
Iteration: 109370, Training Loss: -0.990844
Iteration: 109380, Training Loss: -0.991486
Iteration: 109390, Training Loss: -0.994482
Iteration: 109400, Training Loss: -0.983408
Iteration: 109410, Training Loss: -0.993860
Iteration: 109420, Training Loss: -0.994278
Iteration: 109430, Training Loss: -0.993995
Iteration: 109440, Training Loss: -0.993993
Iteration: 109450, Training Loss: -0.992049
Iteration: 109460, Training Loss: -0.994781
Iteration: 109470, Training Loss: -0.991185
Iteration: 109480, Training Loss: -0.993450
Iteration: 109490, Training Loss: -0.995656
Iteration: 109500, Training Loss: -0.992566
Iteration: 109510, Training Loss: -0.991700
Iteration: 109520, Training Loss: -0.994357
Iteration: 109530, Training Loss: -0.991901
Iteration: 109540, Training Loss: -0.993354
Iteration: 109550, Training Loss: -0.991960
Iteration: 109560, Training Loss: -0.991982
Iteration: 109570, Training Loss: -0.994331
Iteration: 109580, Training Loss: -0.991993
Iteration: 109590, Training Loss: -0.990014
Iteration: 109600, Training Loss: -0.988187
Iteration: 109610, Training Loss: -0.992297
Iteration: 109620, Training Loss: -0.988949
Iteration: 109630, Training Loss: -0.940808
Iteration: 109640, Training Loss: -0.988196
Iteration: 109650, Training Loss: -0.991199
Iteration: 109660, Training Loss: -0.990665
Iteration: 109670, Training Loss: -0.994305
Iteration: 109680, Training Loss: -0.994529
Iteration: 109690, Training Loss: -0.991634
Iteration: 109700, Training Loss: -0.821771
Iteration: 109710, Training Loss: -0.995499
Iteration: 109720, Training Loss: -0.993339
Iteration: 109730, Training Loss: -0.993656
Iteration: 109740, Training Loss: -0.994652
Iteration: 109750, Training Loss: -0.992739
Iteration: 109760, Training Loss: -0.993154
Iteration: 109770, Training Loss: -0.992603
Iteration: 109780, Training Loss: -0.910217
Iteration: 109790, Training Loss: -0.995230
Iteration: 109800, Training Loss: -0.978298
Iteration: 109810, Training Loss: -0.994499
Iteration: 109820, Training Loss: -0.990646
Iteration: 109830, Training Loss: -0.993723
Iteration: 109840, Training Loss: -0.989488
Iteration: 109850, Training Loss: -0.995389
Iteration: 109860, Training Loss: -0.995966
Iteration: 109870, Training Loss: -0.992688
Iteration: 109880, Training Loss: -0.993658
Iteration: 109890, Training Loss: -0.993101
Iteration: 109900, Training Loss: -0.986137
Iteration: 109910, Training Loss: -0.994533
Iteration: 109920, Training Loss: -0.988110
Iteration: 109930, Training Loss: -0.989662
Iteration: 109940, Training Loss: -0.994146
Iteration: 109950, Training Loss: -0.993614
Iteration: 109960, Training Loss: -0.993572
Iteration: 109970, Training Loss: -0.994405
Iteration: 109980, Training Loss: -0.990997
Iteration: 109990, Training Loss: -0.993086
Iteration: 110000, Training Loss: -0.990409
Iteration: 110010, Training Loss: -0.993093
Iteration: 110020, Training Loss: -0.994236
Iteration: 110030, Training Loss: -0.990503
Iteration: 110040, Training Loss: -0.992803
Iteration: 110050, Training Loss: -0.994500
Iteration: 110060, Training Loss: -0.992299
Iteration: 110070, Training Loss: -0.991401
Iteration: 110080, Training Loss: -0.992855
Iteration: 110090, Training Loss: -0.993163
Iteration: 110100, Training Loss: -0.992271
Iteration: 110110, Training Loss: -0.990871
Iteration: 110120, Training Loss: -0.993709
Iteration: 110130, Training Loss: -0.991238
Iteration: 110140, Training Loss: -0.991944
Iteration: 110150, Training Loss: -0.990039
Iteration: 110160, Training Loss: -0.991180
Iteration: 110170, Training Loss: -0.992820
Iteration: 110180, Training Loss: -0.993602
Iteration: 110190, Training Loss: -0.993859
Iteration: 110200, Training Loss: -0.992274
Iteration: 110210, Training Loss: -0.995506
Iteration: 110220, Training Loss: -0.989637
Iteration: 110230, Training Loss: -0.992207
Iteration: 110240, Training Loss: -0.991153
Iteration: 110250, Training Loss: -0.989279
Iteration: 110260, Training Loss: -0.992635
Iteration: 110270, Training Loss: -0.991366
Iteration: 110280, Training Loss: -0.991404
Iteration: 110290, Training Loss: -0.992733
Iteration: 110300, Training Loss: -0.992191
Iteration: 110310, Training Loss: -0.995362
Iteration: 110320, Training Loss: -0.993320
Iteration: 110330, Training Loss: -0.990512
Iteration: 110340, Training Loss: -0.992584
Iteration: 110350, Training Loss: -0.992606
Iteration: 110360, Training Loss: -0.992441
Iteration: 110370, Training Loss: -0.989483
Iteration: 110380, Training Loss: -0.993697
Iteration: 110390, Training Loss: -0.993325
Iteration: 110400, Training Loss: -0.992104
Iteration: 110410, Training Loss: -0.990594
Iteration: 110420, Training Loss: -0.993850
Iteration: 110430, Training Loss: -0.993182
Iteration: 110440, Training Loss: -0.991790
Iteration: 110450, Training Loss: -0.993790
Iteration: 110460, Training Loss: -0.994230
Iteration: 110470, Training Loss: -0.992561
Iteration: 110480, Training Loss: -0.989177
Iteration: 110490, Training Loss: -0.982876
Iteration: 110500, Training Loss: -0.990123
Iteration: 110510, Training Loss: -0.996374
Iteration: 110520, Training Loss: -0.992573
Iteration: 110530, Training Loss: -0.981596
Iteration: 110540, Training Loss: -0.990054
Iteration: 110550, Training Loss: -0.995602
Iteration: 110560, Training Loss: -0.990882
Iteration: 110570, Training Loss: -0.994036
Iteration: 110580, Training Loss: -0.991743
Iteration: 110590, Training Loss: -0.990039
Iteration: 110600, Training Loss: -0.993180
Iteration: 110610, Training Loss: -0.994958
Iteration: 110620, Training Loss: -0.993563
Iteration: 110630, Training Loss: -0.993760
Iteration: 110640, Training Loss: -0.993721
Iteration: 110650, Training Loss: -0.994081
Iteration: 110660, Training Loss: -0.984417
Iteration: 110670, Training Loss: -0.987298
Iteration: 110680, Training Loss: -0.993030
Iteration: 110690, Training Loss: -0.989058
Iteration: 110700, Training Loss: -0.992776
Iteration: 110710, Training Loss: -0.993169
Iteration: 110720, Training Loss: -0.994499
Iteration: 110730, Training Loss: -0.988711
Iteration: 110740, Training Loss: -0.990417
Iteration: 110750, Training Loss: -0.993684
Iteration: 110760, Training Loss: -0.994115
Iteration: 110770, Training Loss: -0.991933
Iteration: 110780, Training Loss: -0.994153
Iteration: 110790, Training Loss: -0.991654
Iteration: 110800, Training Loss: -0.993810
Iteration: 110810, Training Loss: -0.992967
Iteration: 110820, Training Loss: -0.993576
Iteration: 110830, Training Loss: -0.990294
Iteration: 110840, Training Loss: -0.991725
Iteration: 110850, Training Loss: -0.991417
Iteration: 110860, Training Loss: -0.992680
Iteration: 110870, Training Loss: -0.984575
Iteration: 110880, Training Loss: -0.989215
Iteration: 110890, Training Loss: -0.993247
Iteration: 110900, Training Loss: -0.992843
Iteration: 110910, Training Loss: -0.989823
Iteration: 110920, Training Loss: -0.992122
Iteration: 110930, Training Loss: -0.992693
Iteration: 110940, Training Loss: -0.990583
Iteration: 110950, Training Loss: -0.986132
Iteration: 110960, Training Loss: -0.991455
Iteration: 110970, Training Loss: -0.988962
Iteration: 110980, Training Loss: -0.990860
Iteration: 110990, Training Loss: -0.989329
Iteration: 111000, Training Loss: -0.993098
Iteration: 111010, Training Loss: -0.990958
Iteration: 111020, Training Loss: -0.992341
Iteration: 111030, Training Loss: -0.993097
Iteration: 111040, Training Loss: -0.991096
Iteration: 111050, Training Loss: -0.990238
Iteration: 111060, Training Loss: -0.995806
Iteration: 111070, Training Loss: -0.992196
Iteration: 111080, Training Loss: -0.993959
Iteration: 111090, Training Loss: -0.990788
Iteration: 111100, Training Loss: -0.993779
Iteration: 111110, Training Loss: -0.992619
Iteration: 111120, Training Loss: -0.993877
Iteration: 111130, Training Loss: -0.993689
Iteration: 111140, Training Loss: -0.985687
Iteration: 111150, Training Loss: -0.991768
Iteration: 111160, Training Loss: -0.989197
Iteration: 111170, Training Loss: -0.990134
Iteration: 111180, Training Loss: -0.991046
Iteration: 111190, Training Loss: -0.994134
Iteration: 111200, Training Loss: -0.990446
Iteration: 111210, Training Loss: -0.992276
Iteration: 111220, Training Loss: -0.994331
Iteration: 111230, Training Loss: -0.995217
Iteration: 111240, Training Loss: -0.992554
Iteration: 111250, Training Loss: -0.991202
Iteration: 111260, Training Loss: -0.993290
Iteration: 111270, Training Loss: -0.993300
Iteration: 111280, Training Loss: -0.989531
Iteration: 111290, Training Loss: -0.994222
Iteration: 111300, Training Loss: -0.987684
Iteration: 111310, Training Loss: -0.989236
Iteration: 111320, Training Loss: -0.994926
Iteration: 111330, Training Loss: -0.990165
Iteration: 111340, Training Loss: -0.990427
Iteration: 111350, Training Loss: -0.992448
Iteration: 111360, Training Loss: -0.991029
Iteration: 111370, Training Loss: -0.991794
Iteration: 111380, Training Loss: -0.992640
Iteration: 111390, Training Loss: -0.993591
Iteration: 111400, Training Loss: -0.993442
Iteration: 111410, Training Loss: -0.991496
Iteration: 111420, Training Loss: -0.992881
Iteration: 111430, Training Loss: -0.989674
Iteration: 111440, Training Loss: -0.994805
Iteration: 111450, Training Loss: -0.991929
Iteration: 111460, Training Loss: -0.993976
Iteration: 111470, Training Loss: -0.993791
Iteration: 111480, Training Loss: -0.992219
Iteration: 111490, Training Loss: -0.991537
Iteration: 111500, Training Loss: -0.990667
Iteration: 111510, Training Loss: -0.992898
Iteration: 111520, Training Loss: -0.994407
Iteration: 111530, Training Loss: -0.992563
Iteration: 111540, Training Loss: -0.994052
Iteration: 111550, Training Loss: -0.989651
Iteration: 111560, Training Loss: -0.993446
Iteration: 111570, Training Loss: -0.988228
Iteration: 111580, Training Loss: -0.992754
Iteration: 111590, Training Loss: -0.994867
Iteration: 111600, Training Loss: -0.992980
Iteration: 111610, Training Loss: -0.995174
Iteration: 111620, Training Loss: -0.993648
Iteration: 111630, Training Loss: -0.992167
Iteration: 111640, Training Loss: -0.993241
Iteration: 111650, Training Loss: -0.993447
Iteration: 111660, Training Loss: -0.993000
Iteration: 111670, Training Loss: -0.993982
Iteration: 111680, Training Loss: -0.991162
Iteration: 111690, Training Loss: -0.992389
Iteration: 111700, Training Loss: -0.987294
Iteration: 111710, Training Loss: -0.993364
Iteration: 111720, Training Loss: -0.991977
Iteration: 111730, Training Loss: -0.993511
Iteration: 111740, Training Loss: -0.993499
Iteration: 111750, Training Loss: -0.991744
Iteration: 111760, Training Loss: -0.992890
Iteration: 111770, Training Loss: -0.994720
Iteration: 111780, Training Loss: -0.992577
Iteration: 111790, Training Loss: -0.992453
Iteration: 111800, Training Loss: -0.989975
Iteration: 111810, Training Loss: -0.994673
Iteration: 111820, Training Loss: -0.995354
Iteration: 111830, Training Loss: -0.994751
Iteration: 111840, Training Loss: -0.992313
Iteration: 111850, Training Loss: -0.989789
Iteration: 111860, Training Loss: -0.993847
Iteration: 111870, Training Loss: -0.986482
Iteration: 111880, Training Loss: -0.993060
Iteration: 111890, Training Loss: -0.983302
Iteration: 111900, Training Loss: -0.990684
Iteration: 111910, Training Loss: -0.991585
Iteration: 111920, Training Loss: -0.989954
Iteration: 111930, Training Loss: -0.985384
Iteration: 111940, Training Loss: -0.992935
Iteration: 111950, Training Loss: -0.990624
Iteration: 111960, Training Loss: -0.994597
Iteration: 111970, Training Loss: -0.995382
Iteration: 111980, Training Loss: -0.990955
Iteration: 111990, Training Loss: -0.993313
Iteration: 112000, Training Loss: -0.991801
Iteration: 112010, Training Loss: -0.989756
Iteration: 112020, Training Loss: -0.994238
Iteration: 112030, Training Loss: -0.993296
Iteration: 112040, Training Loss: -0.985696
Iteration: 112050, Training Loss: -0.992006
Iteration: 112060, Training Loss: -0.994891
Iteration: 112070, Training Loss: -0.994022
Iteration: 112080, Training Loss: -0.992501
Iteration: 112090, Training Loss: -0.992801
Iteration: 112100, Training Loss: -0.993937
Iteration: 112110, Training Loss: -0.995491
Iteration: 112120, Training Loss: -0.993345
Iteration: 112130, Training Loss: -0.986335
Iteration: 112140, Training Loss: -0.993577
Iteration: 112150, Training Loss: -0.989751
Iteration: 112160, Training Loss: -0.991132
Iteration: 112170, Training Loss: -0.994707
Iteration: 112180, Training Loss: -0.994080
Iteration: 112190, Training Loss: -0.991289
Iteration: 112200, Training Loss: -0.993479
Iteration: 112210, Training Loss: -0.994687
Iteration: 112220, Training Loss: -0.994276
Iteration: 112230, Training Loss: -0.992083
Iteration: 112240, Training Loss: -0.993521
Iteration: 112250, Training Loss: -0.993646
Iteration: 112260, Training Loss: -0.982537
Iteration: 112270, Training Loss: -0.993270
Iteration: 112280, Training Loss: -0.990374
Iteration: 112290, Training Loss: -0.993402
Iteration: 112300, Training Loss: -0.868746
Iteration: 112310, Training Loss: -0.992153
Iteration: 112320, Training Loss: -0.990472
Iteration: 112330, Training Loss: -0.989626
Iteration: 112340, Training Loss: -0.992763
Iteration: 112350, Training Loss: -0.990705
Iteration: 112360, Training Loss: -0.992180
Iteration: 112370, Training Loss: -0.994163
Iteration: 112380, Training Loss: -0.993125
Iteration: 112390, Training Loss: -0.992230
Iteration: 112400, Training Loss: -0.995171
Iteration: 112410, Training Loss: -0.991649
Iteration: 112420, Training Loss: -0.992999
Iteration: 112430, Training Loss: -0.993455
Iteration: 112440, Training Loss: -0.994268
Iteration: 112450, Training Loss: -0.991150
Iteration: 112460, Training Loss: -0.993736
Iteration: 112470, Training Loss: -0.994490
Iteration: 112480, Training Loss: -0.991393
Iteration: 112490, Training Loss: -0.994386
Iteration: 112500, Training Loss: -0.992834
Iteration: 112510, Training Loss: -0.875672
Iteration: 112520, Training Loss: -0.992603
Iteration: 112530, Training Loss: -0.989324
Iteration: 112540, Training Loss: -0.994165
Iteration: 112550, Training Loss: -0.989206
Iteration: 112560, Training Loss: -0.990468
Iteration: 112570, Training Loss: -0.992111
Iteration: 112580, Training Loss: -0.993799
Iteration: 112590, Training Loss: -0.996475
Iteration: 112600, Training Loss: -0.987926
Iteration: 112610, Training Loss: -0.994460
Iteration: 112620, Training Loss: -0.992857
Iteration: 112630, Training Loss: -0.986009
epoch 20 finished
Iteration: 112640, Training Loss: -0.993453
Iteration: 112650, Training Loss: -0.985266
Iteration: 112660, Training Loss: -0.991831
Iteration: 112670, Training Loss: -0.992603
Iteration: 112680, Training Loss: -0.993287
Iteration: 112690, Training Loss: -0.992538
Iteration: 112700, Training Loss: -0.990500
Iteration: 112710, Training Loss: -0.994085
Iteration: 112720, Training Loss: -0.969813
Iteration: 112730, Training Loss: -0.989507
Iteration: 112740, Training Loss: -0.992057
Iteration: 112750, Training Loss: -0.985244
Iteration: 112760, Training Loss: -0.992503
Iteration: 112770, Training Loss: -0.993544
Iteration: 112780, Training Loss: -0.985312
Iteration: 112790, Training Loss: -0.994773
Iteration: 112800, Training Loss: -0.990335
Iteration: 112810, Training Loss: -0.992306
Iteration: 112820, Training Loss: -0.994446
Iteration: 112830, Training Loss: -0.994871
